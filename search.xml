<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/07/08/hello-world/"/>
      <url>/2023/07/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/06/19/shen-du-xue-xi-fan-xiang-chuan-bo/"/>
      <url>/2023/06/19/shen-du-xue-xi-fan-xiang-chuan-bo/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="title-深度学习—反向传播"><a href="#title-深度学习—反向传播" class="headerlink" title="title:深度学习—反向传播"></a>title:深度学习—反向传播</h2><h1 id="深度学习—反向传播"><a href="#深度学习—反向传播" class="headerlink" title="深度学习—反向传播"></a>深度学习—反向传播</h1><p>使用Matt Mazur的例子，来简单推导过程</p><p><img src="https://i.imgtg.com/2023/06/14/OBGvnt.png" alt="OBGvnt.png"></p><p><strong>先初始化权重和偏置量</strong></p><p><img src="https://i.imgtg.com/2023/06/14/OBGt5v.png" alt="OBGt5v.png"></p><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a><strong>前向传播</strong></h2><p><strong>1.计算h<sub>1</sub>所有输入：</strong></p><script type="math/tex; mode=display">net_{h1} = w_{1} * i_{1} + w_{2} * i_{2} + b_{1} * 1</script><p>带入权重与偏置量数据：</p><script type="math/tex; mode=display">net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775</script><p><strong>2.利用logistic函数计算得到h<sub>1</sub>的输出：</strong></p><script type="math/tex; mode=display">out_{h1} =\frac{1}{1+e^{-net_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p><strong>3.利用同样的方法得到</strong></p><script type="math/tex; mode=display">out_{h2} = 0.596884378</script><p><strong>4.使用隐藏层神经元的输出作为输入，用同样的方法给出o<sub>1</sub>的输出：</strong></p><script type="math/tex; mode=display">net_{o_{1}} = w_{5} * out_{h_{1}} + w_{6} * out_{h_{2}} + b_{2} * 1</script><p>带入数据得：</p><script type="math/tex; mode=display">net_{o_{1}} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 =1.105905967</script><p>利用 logistic函数得出其输出为：</p><script type="math/tex; mode=display">out_{o_{1}} = \frac{1}{1+e^{-net_{o_{1}}}} = \frac{1}{1+e^{-1.105905967}} = 0.75136507</script><p>同理可得o<sub>2</sub>的输出：</p><script type="math/tex; mode=display">out_{o_{2}} = 0.772928465</script><p><strong>5.统计所有误差</strong></p><script type="math/tex; mode=display">E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>带入数据，o<sub>1</sub>原始输出为0.01，神经网络的输出为0.75136507，计算出误差为：</p><script type="math/tex; mode=display">E_{o_{1}} = \sum\frac{1}{2}(0.01 - 0.75136507)^2 = 0.298371109</script><p>同理可得:</p><script type="math/tex; mode=display">E_{o_{2}} = 0.023560026</script><p>综上所述：<strong>总误差</strong>：</p><script type="math/tex; mode=display">E_{total} = E_{o_{1}} + E_{o_{2}} = 0.298371109</script><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><p>反过来往前推，对于w5，想知道其改变对总误差有多少影响，可以得到：</p><script type="math/tex; mode=display">\frac{dE_{total}}{dw_{5}}</script><p>通过链式法则可以得到：</p><script type="math/tex; mode=display">\frac{dE_{total}}{dw_{5}} = \frac{dE_{total}}{dout_{o_{1}}} * \frac{dout_{o_{1}}}{dnet_{o_{1}}} * \frac{dnet_{o_{1}}}{dw_{5}}</script><p>其实一直在做的就是如图所示：</p><p><img src="https://i.imgtg.com/2023/06/14/OBG4Kq.png" alt="OBG4Kq.png"></p><p>拆分成每一部分来分析，首先：</p><script type="math/tex; mode=display">E_{total} = \frac{1}{2}(target_{o_{1}}-out_{o_{1}})^2 + \frac{1}{2}(target_{o_{2}}-out_{o_{2}})^2</script><p>对于多变量函数，我们需要使用偏导数来表示函数对每个参数的变化率</p><script type="math/tex; mode=display">\frac{\partial E_{total}}{\partial out_{o_{1}}} = 2*\frac{1}{2}(target_{o_{1}}-out_{o_{1}})^{2-1}*-1 +0</script><script type="math/tex; mode=display">\frac {\partial E_{total}}{\partial out_{o_{1}}} = -(target_{o_{1}}-out_{o_{1}})=-(0.01-0.75136507)=0.74136507</script><p>由logistic函数可知：</p><script type="math/tex; mode=display">out_{o_{1}}=\frac {1}{1+e^{-net_{o_{1}}}}</script><script type="math/tex; mode=display">\frac {\partial out_{o_{1}}}{\partial net_{o_{1}}}=out_{o_{1}}(1-out_{o_{1}})=0.75136507(1-0.75136507)=0.186815602</script><p>由net<sub>o1</sub>输入公式可得：</p><script type="math/tex; mode=display">net_{o_{1}}=w_{5}*out_{h1}+w_{6}*out_{h2}+b_{2}*1</script><script type="math/tex; mode=display">\frac{\partial net_{o_{1}}}{\partial w_{5}}=1*out_{h1}*w_{5}^{(1-1)}+0+0=out_{h1}=0.593269992</script><p>所以可以得到：</p><script type="math/tex; mode=display">\frac{\partial E_{total}}{\partial w_{5}}=\frac{\partial E_{total}}{\partial out_{o_{1}}}*\frac{\partial out_{o_{1}}}{\partial net_{o_{1}}}*\frac{\partial net_{o_{1}}}{\partial w_{5}}=0.74136507*0.186815602*0.593269992=0.082167041</script><p>为了减少误差，可以从当前的权重减去这个值（可以选择一个学习率，比如设置为0.5）得：</p><script type="math/tex; mode=display">w_{5}^+=w_{5}-\eta*\frac{\partial E_{total}}{\partial w_{5}}=0.4-0.5*0.082167041=0.35891648</script><p>同理可得：</p><script type="math/tex; mode=display">w_{6}^+=0.408666186</script><script type="math/tex; mode=display">w_{7}^+=0.511301270</script><script type="math/tex; mode=display">w_{8}^+=0.561370121</script><p>在有新权重导入隐藏层神经元（当继续下面的反向传播算法时，使用原始权重，而不是更新的权重）之后，执行神经网络中的实际更新。</p><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>我们需要算：</p><script type="math/tex; mode=display">\frac{\partial E_{total}}{\partial w_{1}}=\frac{\partial E_{total}}{\partial out_{h1}}*\frac{\partial out_{h1}}{\partial net_{h1}}*\frac{\partial net_{h1}}{\partial w_{1}}</script><p>从图中可以看清楚：</p><p><img src="https://i.imgtg.com/2023/06/14/OBGDSc.png" alt="OBGDSc.png"></p><p>由于：</p><script type="math/tex; mode=display">\frac{\partial E_{total}}{\partial out_{h1}}=\frac{\partial E_{o_{1}}}{\partial out_{h1}}+\frac{E_{o_{2}}}{\partial out_{h1}}</script><p>可拆分计算：</p><script type="math/tex; mode=display">\frac{\partial E_{o_{1}}}{\partial out_{h1}}=\frac{\partial E_{o_{1}}}{\partial net_{o_{1}}}*\frac{\partial net_{o_{1}}}{\partial out_{h1}}</script><script type="math/tex; mode=display">\frac{\partial E_{o_{1}}}{\partial net_{o_{1}}}=\frac{\partial E_{o_{1}}}{\partial out_{o_{1}}}*\frac{\partial out_{o_{1}}}{\partial net_{o_{1}}}=0.74136507*0.186815602=0.148498562</script><p>由于：</p><script type="math/tex; mode=display">net_{o_{1}}=w_{5}*out_{h1}+w_{6}*out_{h2}+b_{2}*1</script><p>可得：</p><script type="math/tex; mode=display">\frac{\partial net_{o_{1}}}{\partial out_{h1}}=w_{5}=0.40</script><p>综合可得：</p><script type="math/tex; mode=display">\frac{\partial E_{o_{1}}}{\partial out_{h1}}=\frac{\partial E_{o_{1}}}{\partial net_{o_{1}}}*\frac{\partial net_{o_{1}}}{\partial out_{h1}}=0.138498562*0.40=0.055399425</script><p>同理可得：</p><script type="math/tex; mode=display">\frac{\partial E_{o_{2}}}{\partial out_{h1}}=-0.019049119</script><p>所以可以得到：</p><script type="math/tex; mode=display">\frac{\partial E_{total}}{\partial out_{h1}}=\frac{\partial E_{o_{1}}}{\partial out_{h1}}+\frac{\partial E_{o_{2}}}{\partial out_{h1}}=0.055399425+-0.019049119=0.036350306</script><p>通过logistic函数：</p><script type="math/tex; mode=display">out_{h1}=\frac{1}{1+e^{-net_{h1}}}</script><p>对net<sub>h1</sub>求偏导：</p><script type="math/tex; mode=display">\frac{\partial out_{h1}}{\partial net_{h1}}=out_{h1}(1-out_{h1})=0.59326999(1-0.59326999)=0.241300709</script><p>由公式可得：</p><script type="math/tex; mode=display">net_{h1}=w_{1}*i_{1}+w_{2}*i_{2}+b_{1}*1</script><script type="math/tex; mode=display">\frac{\partial net_{h1}}{w_{1}}=i_{1}=0.05</script><p>综合可得：</p><script type="math/tex; mode=display">\frac{\partial E_{total}}{\partial w_{1}}=\frac{\partial E_{total}}{out_{h1}}*\frac{\partial out_{h1}}{net_{h1}}*\frac{\partial net_{h1}}{\partial w_{1}}=0.036350306*0.241300709*0.05=0.000438568</script><p>更新w<sub>1</sub>:</p><script type="math/tex; mode=display">w_{1}^+=w_{1}-\eta*\frac{\partial E_{total}}{\partial w_{1}}=0.15-0.5*0.000438568=0.149780716</script><p>同理可得：</p><script type="math/tex; mode=display">w_{2}^+=0.19956143</script><script type="math/tex; mode=display">w_{3}^+=0.24975114</script><script type="math/tex; mode=display">w_{4}^+=0.29950229</script><p>最后，更新了所有的权重！ 当最初前馈传播时输入为0.05和0.1，网络上的误差是0.298371109。 在第一轮反向传播之后，总误差现在下降到0.291027924</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
